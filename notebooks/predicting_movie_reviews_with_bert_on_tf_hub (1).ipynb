{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0a4mTk9o1Qg"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "#Predicting Movie Review Sentiment with BERT on TF Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiYrZKaHwV81"
   },
   "source": [
    "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
    "\n",
    "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
    "\n",
    "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cp5wfXDx5SPH"
   },
   "source": [
    "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jviywGyWyKsA",
    "outputId": "166f3005-d219-404f-b201-2a0b75480360"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhbGEfwgdEtw"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVB3eOcjxxm1"
   },
   "source": [
    "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
    "\n",
    "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
    "\n",
    "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"Hello it's monday\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "US_EAnICvP7f",
    "outputId": "7780a032-31d4-4794-e6aa-664a5d2ae7dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: output-test *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "OUTPUT_DIR = 'output-test'#@param {type:\"string\"}\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = False #@param {type:\"boolean\"}\n",
    "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = False #@param {type:\"boolean\"}\n",
    "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "  try:\n",
    "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "  except:\n",
    "    # Doesn't matter if the directory didn't exist\n",
    "    pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmFYvkylMwXn"
   },
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MC_w8SRqN0fr"
   },
   "source": [
    "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fom_ff20gyy6"
   },
   "outputs": [],
   "source": [
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "  return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    3750\n",
      "0    3750\n",
      "Name: Tag, dtype: int64\n",
      "1    1250\n",
      "0    1250\n",
      "Name: Tag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_train = pd.read_csv(\"Data/trainIMDB7000.csv\", sep =\";\")\n",
    "df_test = pd.DataFrame()\n",
    "df_test = pd.read_csv(\"Data/testIMDB3000.csv\", sep =\";\")\n",
    "print(df_train['Tag'].value_counts())\n",
    "print(df_test['Tag'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    #s = text.split()\n",
    "    #s = [t for t in s if (t not in stopwords.words('english')) or (t not in other_stopwords)]\n",
    "    #s = [t for t in s if t not in other_stopwords]\n",
    "    #s = \" \".join(s)\n",
    "    no_tabs = text.lower().replace('\\t', ' ')\n",
    "    remove_tag = re.sub(r'@[A-Za-z0-9]+', \"\", no_tabs)\n",
    "    remove_url = re.sub(r'https?://[A-Za-z0-9./]+', \"\", remove_tag)\n",
    "    alpha_only = re.sub(\"<br\\s*/><br\\s*/>\", \" \", no_tabs)\n",
    "    #multi_spaces = re.sub(\"[^a-zA-Z\\.\\!]\", \" \", alpha_only) !!!!!! temporary\n",
    "    multi_spaces = re.sub(\"[^a-zA-Z\\!]\", \" \", alpha_only)\n",
    "    text_clean = re.sub(\" +\", \" \", multi_spaces);\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135"
   },
   "outputs": [],
   "source": [
    "train_init, test_init = download_and_load_datasets()\n",
    "train_init.drop([\"sentiment\"], inplace=True, axis=1)\n",
    "test_init.drop([\"sentiment\"], inplace=True, axis=1)\n",
    "train_init.columns = [\"Phrase\", \"Tag\"]\n",
    "test_init.columns = [\"Phrase\", \"Tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "                                              Phrase  Tag\n",
      "0  Zodiac Killer. 1 out of 10. Worst acting ever....    0\n",
      "1  The film is poorly casted, except for some fam...    0\n",
      "2  A flying saucer manned (literally) by a crew o...    1\n",
      "3  Esther Williams gets her first post MGM starri...    1\n",
      "4  I was always a big fan of this movie, first of...    1\n"
     ]
    }
   ],
   "source": [
    "print(train_init.shape)\n",
    "print(train_init.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_init.head())\n",
    "df_train_clean = train_init.copy()\n",
    "df_train_clean.Phrase = [normalize_text(row) for row in train_init.Phrase]\n",
    "print(df_train_clean.head())\n",
    "df_test_clean = test_init.copy()\n",
    "df_test_clean.Phrase = [normalize_text(row) for row in df_test_clean.Phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_init = df_train.reindex(np.random.permutation(df_train.index))\n",
    "test_init = df_test.reindex(np.random.permutation(df_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "                                              Phrase  Tag\n",
      "0  The emergence of Quentin Tarantino and his dub...    0\n",
      "1  THE PLOT: A trucker (Kristofferson) battles a ...    0\n",
      "2  Hm. Where do I start? I usually ignore whateve...    0\n",
      "3  Well another shootem up. Typical run around fi...    0\n",
      "4  The Polar Express. Director Robert Zemeckis, I...    0\n",
      "The emergence of Quentin Tarantino and his dubious influence on the likes of Guy Ritchie may have triggered the wave of appalling British gangster flicks we've been bequeathed over the past few years, but one of our most famous acting exports only serves to perpetuate the cycle by lending his considerable name to trash like this. I only wish he'd taken a moment to consider before choosing this project for the same reasons of personal gain he admits he often employs. It's not only stifling HIS talent, but possibly the promise of future originality from British films. <br /><br />Not one of this film's characters are likeable or even remotely realistic, and the dialogue consists of the usual empty threats and colourful language. Caine doesn't give the material any more effort than it deserves, either. If this was meant to be in the style of a tragic fall from grace a la \"King Lear\", it would've helped immensely had I cared about the ultimate fate of the principals, instead of just wishing that they'd get mired in the quicksand of life and dragged under almost immediately.<br /><br />\n",
      "1    12500\n",
      "0    12500\n",
      "Name: Tag, dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-35b59cd1eeb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhrase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhrase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_init.shape)\n",
    "print(test_init.head())\n",
    "print(test_init.Phrase[0])\n",
    "print(train_init.Tag.value_counts())\n",
    "print(df_test.shape)\n",
    "print(df_test.head())\n",
    "print(df_test.Phrase[0])\n",
    "print(df_train.Tag.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw_F488eixTV"
   },
   "outputs": [],
   "source": [
    "train = train_init#.sample(10000)\n",
    "test = test_init#.sample(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('trainIMDBkeras7000.csv',sep=\";\")\n",
    "test.to_csv('testIMDBkeras3000.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "prRQM8pDi8xI",
    "outputId": "34445cb8-2be0-4379-fdbc-7794091f6049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    5036\n",
      "0    4964\n",
      "Name: Tag, dtype: int64\n",
      "(3000, 2)\n",
      "                                                  Phrase  Tag\n",
      "23318  Mockney comes to Brighton; despite a poor rece...    0\n",
      "9812   Oh yes, I admit I have made myself guilty of t...    0\n",
      "3932   \"Wagons East\" was a big disappointment for me....    0\n",
      "18527  I do not think I am alone when I say that 2005...    1\n",
      "21097  This has got to be the most god-awful piece of...    0\n",
      "0    1506\n",
      "1    1494\n",
      "Name: Tag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train.columns\n",
    "print(train.Tag.value_counts())\n",
    "print(test.shape)\n",
    "print(test.head())\n",
    "#print(test['Phrase'][0])\n",
    "print(test.Tag.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'Phrase'\n",
    "LABEL_COLUMN = 'Tag'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V399W0rqNJ-Z"
   },
   "source": [
    "#Data Preprocessing\n",
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "- `label` is the label for our example, i.e. True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9gEt5SmM6i6"
   },
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCZWZtKxObjh"
   },
   "source": [
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "\n",
    "\n",
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "Happily, we don't have to worry about most of these details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMWiDtpyQSoU"
   },
   "source": [
    "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IhJSe0QHNG7U",
    "outputId": "20b28cc7-3cb3-4ce6-bfff-a7847ce3bbaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'.\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 281.62MB\n",
      "INFO:tensorflow:Downloaded https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1, Total size: 421.86MB\n",
      "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4oFkhpZBDKm"
   },
   "source": [
    "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "dsBo6RCtQmwx",
    "outputId": "9af8c917-90ec-4fe9-897b-79dc89ca88e1"
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OEzfFIt6GIc"
   },
   "source": [
    "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1261
    },
    "colab_type": "code",
    "id": "LL5W8gEGRTAf",
    "outputId": "65001dda-155b-48fc-b5fc-1e4cabc8dfbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 25000\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] zodiac killer . 1 out of 10 . worst acting ever . no really worst acting ever . david hess ( last house on the left . no the one from the seventies . rent it it ' s really good ) is the worst of the bunch ( pretty stiff competition but he is amazingly god - awful . ) one would be hard pressed to find a home movie participant with such an awkward camera presence . the film actually sc ##ree ##ches to a stunning painful halt when he is on the screen . < br / > < br / > not that the film actually has any red ##eem ##ing qualities for mr . hess to ruin . it is filmed [SEP]\n",
      "INFO:tensorflow:input_ids: 101 28501 6359 1012 1015 2041 1997 2184 1012 5409 3772 2412 1012 2053 2428 5409 3772 2412 1012 2585 23484 1006 2197 2160 2006 1996 2187 1012 2053 1996 2028 2013 1996 26232 1012 9278 2009 2009 1005 1055 2428 2204 1007 2003 1996 5409 1997 1996 9129 1006 3492 10551 2971 2021 2002 2003 29350 2643 1011 9643 1012 1007 2028 2052 2022 2524 4508 2000 2424 1037 2188 3185 13180 2007 2107 2019 9596 4950 3739 1012 1996 2143 2941 8040 9910 8376 2000 1037 14726 9145 9190 2043 2002 2003 2006 1996 3898 1012 1026 7987 1013 1028 1026 7987 1013 1028 2025 2008 1996 2143 2941 2038 2151 2417 21564 2075 11647 2005 2720 1012 23484 2000 10083 1012 2009 2003 6361 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] the film is poorly caste ##d , except for some familiar old hollywood names . other performances by unknown names ( i . e . , jennifer gabrielle ) are un ##ins ##pi ##ring . i have seen other films by this director , unfortunately this is one of his worst . perhaps this is a reflection of the screenplay ? < br / > < br / > in a positive note , kim bass ##inger ' s and pat mori ##ta ' s performance saved the movie from oblivion . i enjoyed pat more in karate kid , though . there are many good movies to see , and in short , this one is not one of them . save your money and [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 2143 2003 9996 14542 2094 1010 3272 2005 2070 5220 2214 5365 3415 1012 2060 4616 2011 4242 3415 1006 1045 1012 1041 1012 1010 7673 16988 1007 2024 4895 7076 8197 4892 1012 1045 2031 2464 2060 3152 2011 2023 2472 1010 6854 2023 2003 2028 1997 2010 5409 1012 3383 2023 2003 1037 9185 1997 1996 9000 1029 1026 7987 1013 1028 1026 7987 1013 1028 1999 1037 3893 3602 1010 5035 3321 9912 1005 1055 1998 6986 22993 2696 1005 1055 2836 5552 1996 3185 2013 24034 1012 1045 5632 6986 2062 1999 16894 4845 1010 2295 1012 2045 2024 2116 2204 5691 2000 2156 1010 1998 1999 2460 1010 2023 2028 2003 2025 2028 1997 2068 1012 3828 2115 2769 1998 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] a flying sauce ##r manned ( literally ) by a crew of about 20 male space explorers travels hundreds of millions of light years from earth to check in on a colony founded some 25 years ago on a ' forbidden planet . ' what they find is a robot more advanced than anything im ##agi ##nable on earth , a beautiful and totally socially in ##ept young woman , and her father , a hermit phil ##ologist haunted by more than the demons of the ancient civilization he has immersed himself in . < br / > < br / > on the surface , this story is a pulp sci ##fi murder mystery . some compare it to shakespeare ' s tempest , but [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1037 3909 12901 2099 15371 1006 6719 1007 2011 1037 3626 1997 2055 2322 3287 2686 19264 7930 5606 1997 8817 1997 2422 2086 2013 3011 2000 4638 1999 2006 1037 5701 2631 2070 2423 2086 3283 2006 1037 1005 10386 4774 1012 1005 2054 2027 2424 2003 1037 8957 2062 3935 2084 2505 10047 22974 22966 2006 3011 1010 1037 3376 1998 6135 14286 1999 23606 2402 2450 1010 1998 2014 2269 1010 1037 24308 6316 8662 11171 2011 2062 2084 1996 7942 1997 1996 3418 10585 2002 2038 26275 2370 1999 1012 1026 7987 1013 1028 1026 7987 1013 1028 2006 1996 3302 1010 2023 2466 2003 1037 16016 16596 8873 4028 6547 1012 2070 12826 2009 2000 8101 1005 1055 22553 1010 2021 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] esther williams gets her first post mgm starring role and gets off < br / > < br / > to a good start . this film is a well acted entertaining suspense < br / > < br / > with a mature theme that would be repeated a million times more < br / > < br / > in the future - innocent girl stalked creepy woman hate ##r . esther < br / > < br / > looks great and if she wanted to , probably could have gone on to < br / > < br / > do more and better films but according to her autobiography , < br / > < br / > pretty much gave [SEP]\n",
      "INFO:tensorflow:input_ids: 101 14631 3766 4152 2014 2034 2695 15418 4626 2535 1998 4152 2125 1026 7987 1013 1028 1026 7987 1013 1028 2000 1037 2204 2707 1012 2023 2143 2003 1037 2092 6051 14036 23873 1026 7987 1013 1028 1026 7987 1013 1028 2007 1037 9677 4323 2008 2052 2022 5567 1037 2454 2335 2062 1026 7987 1013 1028 1026 7987 1013 1028 1999 1996 2925 1011 7036 2611 15858 17109 2450 5223 2099 1012 14631 1026 7987 1013 1028 1026 7987 1013 1028 3504 2307 1998 2065 2016 2359 2000 1010 2763 2071 2031 2908 2006 2000 1026 7987 1013 1028 1026 7987 1013 1028 2079 2062 1998 2488 3152 2021 2429 2000 2014 10828 1010 1026 7987 1013 1028 1026 7987 1013 1028 3492 2172 2435 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] i was always a big fan of this movie , first of all have you seen the cast , the acting is superb and help make this movie move along very well . cy ##bill shepherd was given great reviews for her role , and they were well deserved . the beginning of this movie starts in the past when corinne jeff ##ries ( cy ##bill ) whose picture - perfect marriage comes to a shattering halt when her husband louie dies unexpectedly . fortunately , lou ##se gets a second shot at life when he agrees to be \" recycled \" back to earth as the newborn alex finch ( robert down ##ey , jr ) . alex goes on to live his new life [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2001 2467 1037 2502 5470 1997 2023 3185 1010 2034 1997 2035 2031 2017 2464 1996 3459 1010 1996 3772 2003 21688 1998 2393 2191 2023 3185 2693 2247 2200 2092 1012 22330 24457 11133 2001 2445 2307 4391 2005 2014 2535 1010 1998 2027 2020 2092 10849 1012 1996 2927 1997 2023 3185 4627 1999 1996 2627 2043 26879 5076 5134 1006 22330 24457 1007 3005 3861 1011 3819 3510 3310 2000 1037 21797 9190 2043 2014 3129 17438 8289 14153 1012 14599 1010 10223 3366 4152 1037 2117 2915 2012 2166 2043 2002 10217 2000 2022 1000 22207 1000 2067 2000 3011 2004 1996 20662 4074 16133 1006 2728 2091 3240 1010 3781 1007 1012 4074 3632 2006 2000 2444 2010 2047 2166 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:Writing example 10000 of 25000\n",
      "INFO:tensorflow:Writing example 20000 of 25000\n",
      "INFO:tensorflow:Writing example 0 of 25000\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] the emergence of quentin tara ##ntino and his dubious influence on the likes of guy ritchie may have triggered the wave of app ##all ##ing british gangster flick ##s we ' ve been bequeathed over the past few years , but one of our most famous acting exports only serves to per ##pet ##uate the cycle by lending his considerable name to trash like this . i only wish he ' d taken a moment to consider before choosing this project for the same reasons of personal gain he admits he often employs . it ' s not only st ##if ##ling his talent , but possibly the promise of future original ##ity from british films . < br / > < br / > not [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 14053 1997 15969 10225 25318 1998 2010 22917 3747 2006 1996 7777 1997 3124 20404 2089 2031 13330 1996 4400 1997 10439 8095 2075 2329 20067 17312 2015 2057 1005 2310 2042 27180 2058 1996 2627 2261 2086 1010 2021 2028 1997 2256 2087 3297 3772 14338 2069 4240 2000 2566 22327 20598 1996 5402 2011 18435 2010 6196 2171 2000 11669 2066 2023 1012 1045 2069 4299 2002 1005 1040 2579 1037 2617 2000 5136 2077 10549 2023 2622 2005 1996 2168 4436 1997 3167 5114 2002 14456 2002 2411 13495 1012 2009 1005 1055 2025 2069 2358 10128 2989 2010 5848 1010 2021 4298 1996 4872 1997 2925 2434 3012 2013 2329 3152 1012 1026 7987 1013 1028 1026 7987 1013 1028 2025 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] the plot : a truck ##er ( kris ##to ##ffer ##son ) battles a corrupt sheriff ( borg ##nine ) by getting his fellow truck ##ers to band together and form an un ##sto ##ppa ##ble convoy that stretches for miles and soon creates a national media frenzy . < br / > < br / > the negative : the film ' s setup is weak and the ending even weaker . it has all the good - ole - boy / truck ##er cl ##iche ##s without adding anything new in the process . it ends up making smoke ##y and the bandit look brilliant and inspired . kris ##to ##ffer ##son is much too laid back for a leading man role and cannot [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 5436 1024 1037 4744 2121 1006 19031 3406 12494 3385 1007 7465 1037 13593 6458 1006 28709 19105 1007 2011 2893 2010 3507 4744 2545 2000 2316 2362 1998 2433 2019 4895 16033 13944 3468 9549 2008 14082 2005 2661 1998 2574 9005 1037 2120 2865 21517 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 4997 1024 1996 2143 1005 1055 16437 2003 5410 1998 1996 4566 2130 15863 1012 2009 2038 2035 1996 2204 1011 15589 1011 2879 1013 4744 2121 18856 17322 2015 2302 5815 2505 2047 1999 1996 2832 1012 2009 4515 2039 2437 5610 2100 1998 1996 25334 2298 8235 1998 4427 1012 19031 3406 12494 3385 2003 2172 2205 4201 2067 2005 1037 2877 2158 2535 1998 3685 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] hm . where do i start ? i usually ignore whatever rating im ##db has when looking up a movie because i think i might like it anyway or whatever and i should at least give it a chance , but this time i wish i ' d paid attention . < br / > < br / > i know some people liked it , and i ' m not trying to say that they shouldn ' t . it was semi - amusing at some parts . but if you ' re like me and you don ' t like watching cats pr ##an ##cing around in the under ##growth for 20 minutes , random fast motion cloud scenes , dogs barking in cages [SEP]\n",
      "INFO:tensorflow:input_ids: 101 20287 1012 2073 2079 1045 2707 1029 1045 2788 8568 3649 5790 10047 18939 2038 2043 2559 2039 1037 3185 2138 1045 2228 1045 2453 2066 2009 4312 2030 3649 1998 1045 2323 2012 2560 2507 2009 1037 3382 1010 2021 2023 2051 1045 4299 1045 1005 1040 3825 3086 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2113 2070 2111 4669 2009 1010 1998 1045 1005 1049 2025 2667 2000 2360 2008 2027 5807 1005 1056 1012 2009 2001 4100 1011 19142 2012 2070 3033 1012 2021 2065 2017 1005 2128 2066 2033 1998 2017 2123 1005 1056 2066 3666 8870 10975 2319 6129 2105 1999 1996 2104 26982 2005 2322 2781 1010 6721 3435 4367 6112 5019 1010 6077 19372 1999 27157 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] well another shoot ##em up . typical run around film with guns , revenge , and violence . not much of a story . in fact i forgot most of what this film is about . don ' t rent this one . i think the exercise info ##mer ##tial would be more entertaining during late night . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2092 2178 5607 6633 2039 1012 5171 2448 2105 2143 2007 4409 1010 7195 1010 1998 4808 1012 2025 2172 1997 1037 2466 1012 1999 2755 1045 9471 2087 1997 2054 2023 2143 2003 2055 1012 2123 1005 1056 9278 2023 2028 1012 1045 2228 1996 6912 18558 5017 20925 2052 2022 2062 14036 2076 2397 2305 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] the polar express . director robert ze ##me ##cki ##s , i love back to the future , forrest gum ##p , contact , and who framed roger rabbit ( no question mark after that movie title ! ! ) . and tom hank ##s , one of my favorite actors . the reviews of this movie were almost unanimous saying that this is an instant holiday classic . ebert & roe ##per give it two enthusiastic thumbs up ! even ebert ' s written review gave it a full four stars ! wow . . . ok . . . this i gotta see ! but wait . . . the motion capture used looks really weird . hmm . . . maybe i ' [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 11508 4671 1012 2472 2728 27838 4168 18009 2015 1010 1045 2293 2067 2000 1996 2925 1010 16319 16031 2361 1010 3967 1010 1998 2040 10366 5074 10442 1006 2053 3160 2928 2044 2008 3185 2516 999 999 1007 1012 1998 3419 9180 2015 1010 2028 1997 2026 5440 5889 1012 1996 4391 1997 2023 3185 2020 2471 13604 3038 2008 2023 2003 2019 7107 6209 4438 1012 22660 1004 20944 4842 2507 2009 2048 14727 16784 2039 999 2130 22660 1005 1055 2517 3319 2435 2009 1037 2440 2176 3340 999 10166 1012 1012 1012 7929 1012 1012 1012 2023 1045 10657 2156 999 2021 3524 1012 1012 1012 1996 4367 5425 2109 3504 2428 6881 1012 17012 1012 1012 1012 2672 1045 1005 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:Writing example 10000 of 25000\n",
      "INFO:tensorflow:Writing example 20000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "#Creating a model\n",
    "\n",
    "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for politeness data.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpE0ZIDOCQzE"
   },
   "source": [
    "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnH-AnOQ9KKW"
   },
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjwJ4bTeWXD8"
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emHf9GhfWBZ_"
   },
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEJldMr3WYZa"
   },
   "outputs": [],
   "source": [
    "# Specify output directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "q_WebpS1X97v",
    "outputId": "1648932a-7391-49d3-8af7-52d514e226e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'output-test', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f520b58f3c8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOO3RfG1DYLo"
   },
   "source": [
    "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Pv2bAlOX_-K"
   },
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6Nukby2EB6-"
   },
   "source": [
    "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "nucD4gluYJmK",
    "outputId": "5d728e72-4631-42bf-c48d-3f51d4b968ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supercalculateur/environment/default_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from output-test/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into output-test/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.73726887, step = 0\n",
      "INFO:tensorflow:global_step/sec: 1.9122\n",
      "INFO:tensorflow:loss = 0.5044448, step = 100 (52.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.37167\n",
      "INFO:tensorflow:loss = 0.13499644, step = 200 (42.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36522\n",
      "INFO:tensorflow:loss = 0.51838756, step = 300 (42.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36444\n",
      "INFO:tensorflow:loss = 0.46407074, step = 400 (42.293 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into output-test/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.25737\n",
      "INFO:tensorflow:loss = 0.20553747, step = 500 (44.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36169\n",
      "INFO:tensorflow:loss = 0.24974205, step = 600 (42.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36265\n",
      "INFO:tensorflow:loss = 0.30789402, step = 700 (42.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.3604\n",
      "INFO:tensorflow:loss = 0.24449696, step = 800 (42.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36195\n",
      "INFO:tensorflow:loss = 0.11918483, step = 900 (42.338 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into output-test/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.25482\n",
      "INFO:tensorflow:loss = 0.08788774, step = 1000 (44.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36348\n",
      "INFO:tensorflow:loss = 0.07199459, step = 1100 (42.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36162\n",
      "INFO:tensorflow:loss = 0.08500516, step = 1200 (42.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36133\n",
      "INFO:tensorflow:loss = 0.08728362, step = 1300 (42.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.35869\n",
      "INFO:tensorflow:loss = 0.18725872, step = 1400 (42.396 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1500 into output-test/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.25184\n",
      "INFO:tensorflow:loss = 0.16252278, step = 1500 (44.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36102\n",
      "INFO:tensorflow:loss = 0.046142053, step = 1600 (42.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36063\n",
      "INFO:tensorflow:loss = 0.040531386, step = 1700 (42.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36015\n",
      "INFO:tensorflow:loss = 0.035766363, step = 1800 (42.370 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.3606\n",
      "INFO:tensorflow:loss = 0.0027697387, step = 1900 (42.362 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into output-test/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.25264\n",
      "INFO:tensorflow:loss = 0.0035785704, step = 2000 (44.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.35967\n",
      "INFO:tensorflow:loss = 0.012136638, step = 2100 (42.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.36027\n",
      "INFO:tensorflow:loss = 0.0020855973, step = 2200 (42.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.35949\n",
      "INFO:tensorflow:loss = 0.0025008963, step = 2300 (42.382 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2343 into output-test/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.17194477.\n",
      "Training took time  0:17:37.926443\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmbLTVniARy3"
   },
   "source": [
    "Now let's use our test data to see how well our model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIhejfpyJ8Bx"
   },
   "outputs": [],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "PPVEXhNjYXC-",
    "outputId": "dd5482cd-c558-465f-c854-ec11a0175316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-05-08:18:26\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from output-test/model.ckpt-2343\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-05-08:20:10\n",
      "INFO:tensorflow:Saving dict for global step 2343: auc = 0.88587993, eval_accuracy = 0.88588, f1_score = 0.88476914, false_negatives = 1547.0, false_positives = 1306.0, global_step = 2343, loss = 0.47305262, precision = 0.893466, recall = 0.87624, true_negatives = 11194.0, true_positives = 10953.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2343: output-test/model.ckpt-2343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.88587993,\n",
       " 'eval_accuracy': 0.88588,\n",
       " 'f1_score': 0.88476914,\n",
       " 'false_negatives': 1547.0,\n",
       " 'false_positives': 1306.0,\n",
       " 'loss': 0.47305262,\n",
       " 'precision': 0.893466,\n",
       " 'recall': 0.87624,\n",
       " 'true_negatives': 11194.0,\n",
       " 'true_positives': 10953.0,\n",
       " 'global_step': 2343}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueKsULteiz1B"
   },
   "source": [
    "Now let's write code to make predictions on new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsrbTD2EJTVl"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def getPrediction(in_sentences):\n",
    "    #labels = [\"Negative\", \"Positive\"]\n",
    "  labels = [0, 1]\n",
    "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "  predictions = estimator.predict(predict_input_fn)\n",
    "  return [(sentence, np.exp(prediction['probabilities']), labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-thbodgih_VJ"
   },
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\",\n",
    "  \"I am happy\",\n",
    "  \"I am happy!!!\",\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "QrZmvZySKQTm",
    "outputId": "3891fafb-a460-4eb8-fa6c-335a5bbc10e5"
   },
   "outputs": [],
   "source": [
    "predictions = getPrediction(pred_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pred = []\n",
    "for s, prob, pred in predictions:\n",
    "    if pred != test_init[\"Phrase\"=s].Tag:\n",
    "        false_pred.append([s, pred, prob])\n",
    "print(false_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXkRiEBUqN3n"
   },
   "source": [
    "Voila! We have a sentiment classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ERkTE8-7oQLZ",
    "outputId": "26c33224-dc2c-4b3d-f7b4-ac3ef0a58b27"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting Movie Reviews with BERT on TF Hub.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
