{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0a4mTk9o1Qg"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "#Predicting Movie Review Sentiment with BERT on TF Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiYrZKaHwV81"
   },
   "source": [
    "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
    "\n",
    "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
    "\n",
    "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cp5wfXDx5SPH"
   },
   "source": [
    "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jviywGyWyKsA",
    "outputId": "166f3005-d219-404f-b201-2a0b75480360"
   },
   "outputs": [],
   "source": [
    "#!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhbGEfwgdEtw"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVB3eOcjxxm1"
   },
   "source": [
    "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
    "\n",
    "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
    "\n",
    "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "US_EAnICvP7f",
    "outputId": "7780a032-31d4-4794-e6aa-664a5d2ae7dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: output-amazon-testexport *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "OUTPUT_DIR = 'output-amazon-testexport'#@param {type:\"string\"}\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = False #@param {type:\"boolean\"}\n",
    "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = False #@param {type:\"boolean\"}\n",
    "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "  try:\n",
    "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "  except:\n",
    "    # Doesn't matter if the directory didn't exist\n",
    "    pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmFYvkylMwXn"
   },
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MC_w8SRqN0fr"
   },
   "source": [
    "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    #s = text.split()\n",
    "    #s = [t for t in s if (t not in stopwords.words('english')) or (t not in other_stopwords)]\n",
    "    #s = [t for t in s if t not in other_stopwords]\n",
    "    #s = \" \".join(s)\n",
    "    no_tabs = text.lower().replace('\\t', ' ')\n",
    "    remove_tag = re.sub(r'@[A-Za-z0-9]+', \"\", no_tabs)\n",
    "    remove_url = re.sub(r'https?://[A-Za-z0-9./]+', \"\", remove_tag)\n",
    "    alpha_only = re.sub(\"<br\\s*/><br\\s*/>\", \" \", no_tabs)\n",
    "    #multi_spaces = re.sub(\"[^a-zA-Z\\.\\!]\", \" \", alpha_only) !!!!!! temporary\n",
    "    multi_spaces = re.sub(\"[^a-zA-Z\\!]\", \" \", alpha_only)\n",
    "    text_clean = re.sub(\" +\", \" \", multi_spaces);\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Good morning, how are you?\"\n",
    "print(\"Good morning\" in string)\n",
    "print(re.sub(\"Good morning\",\"\", string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fom_ff20gyy6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1689188, 2)\n",
      "   Tag                                             Phrase\n",
      "0  5.0  We got this GPS for my husband who is an (OTR)...\n",
      "1  1.0  I'm a professional OTR truck driver, and I bou...\n",
      "2  3.0  Well, what can I say.  I've had this unit in m...\n",
      "3  2.0  Not going to write a long review, even thought...\n",
      "4  1.0  I've had mine for a year and here's what we go...\n",
      "5.0    1009026\n",
      "4.0     347041\n",
      "3.0     142257\n",
      "1.0     108725\n",
      "2.0      82139\n",
      "Name: Tag, dtype: int64\n",
      "   Tag                                             Phrase\n",
      "0  5.0  We got this GPS for my husband who is an (OTR)...\n",
      "1  1.0  I'm a professional OTR truck driver, and I bou...\n",
      "2  3.0  Well, what can I say.  I've had this unit in m...\n",
      "3  2.0  Not going to write a long review, even thought...\n",
      "4  1.0  I've had mine for a year and here's what we go...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open('Data/amazonelectronics.json') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    " \n",
    "# Print loaded json object\n",
    "#print(data[0:2])\n",
    "#print(type(data))\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "df_elec = pd.DataFrame.from_dict(json_normalize(data), orient='columns')\n",
    "df_elec.drop(['asin', 'helpful', 'reviewTime', 'reviewerID', 'reviewerName', 'summary', 'unixReviewTime'], axis=1, inplace=True)\n",
    "df_elec.columns = ['Tag', 'Phrase']\n",
    "print(df_elec.shape)\n",
    "print(df_elec.head())\n",
    "print(df_elec.Tag.value_counts())\n",
    "print(df_elec.head())\n",
    "df_elec_clean = df_elec.copy()\n",
    "#df_elec_clean.Phrase = [normalize_text(row) for row in df_elec.Phrase]\n",
    "#print(df_elec_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5    5000\n",
      "0.0     500\n",
      "1.0     500\n",
      "Name: Tag, dtype: int64\n",
      "1    500\n",
      "0    500\n",
      "Name: Tag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# négatif : 1, neutre : 2, 3, 4, positif : 5\n",
    "df_elec_5 = df_elec_clean[df_elec_clean.Tag == 5.0]\n",
    "df_elec_4 = df_elec_clean[df_elec_clean.Tag == 4.0]\n",
    "df_elec_3 = df_elec_clean[df_elec_clean.Tag == 3.0]\n",
    "df_elec_2 = df_elec_clean[df_elec_clean.Tag == 2.0]\n",
    "df_elec_1 = df_elec_clean[df_elec_clean.Tag == 1.0]\n",
    "df_elec_pos_sample = df_elec_5.sample(n=500, random_state=42)\n",
    "df_elec_neg_sample = df_elec_1.sample(n=500, random_state=42)\n",
    "df_elec_neu_sample = pd.concat([df_elec_2, df_elec_3, df_elec_4]).sample(n=5000, random_state=42)\n",
    "df_small_elec = pd.concat([df_elec_pos_sample, df_elec_neg_sample, df_elec_neu_sample])\n",
    "df_small_elec['Tag'].replace(1.0, 0, inplace=True)\n",
    "df_small_elec['Tag'].replace(2.0, 0.5, inplace=True)\n",
    "df_small_elec['Tag'].replace(3.0, 0.5, inplace=True)\n",
    "df_small_elec['Tag'].replace(4.0, 0.5, inplace=True)\n",
    "df_small_elec['Tag'].replace(5.0, 1, inplace=True)\n",
    "df_binary_small_elec = pd.concat([df_elec_pos_sample, df_elec_neg_sample])\n",
    "df_binary_small_elec['Tag'].replace(1.0, 0, inplace=True)\n",
    "df_binary_small_elec['Tag'].replace(5.0, 1, inplace=True)\n",
    "df_binary_small_elec.Tag = df_binary_small_elec.Tag.astype(int)\n",
    "#df_small_elec.Tag = df_small_elec.Tag.astype(int)\n",
    "print(df_small_elec.Tag.value_counts())\n",
    "print(df_binary_small_elec.Tag.value_counts())\n",
    "#df_binary_small_elec.to_csv(\"dataAmazonElecSmall.csv\", sep=\";\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary_small_elec.to_csv(\"data_en.csv\", sep=\";\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2)\n",
      "(200, 2)\n",
      "         Tag                                             Phrase\n",
      "756047     0  It was WONDERFUL ...at first. But it was withi...\n",
      "275863     0  I noticed that a bunch of people recently had ...\n",
      "1301264    0  It took quite a bit of troubleshooting and tim...\n",
      "1191914    0  Since 5 month after purchase it started to pro...\n",
      "1383043    1  Very impressed. This product just works. Had n...\n",
      "0    401\n",
      "1    399\n",
      "Name: Tag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_init = df_binary_small_elec.sample(n=800, random_state=42)\n",
    "test_init = df_binary_small_elec.loc[~df_binary_small_elec.index.isin(train_init.index)]\n",
    "print(train_init.shape)\n",
    "print(test_init.shape)\n",
    "print(train_init.head())\n",
    "print(train_init.Tag.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/Pet_Supplies.json') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    " \n",
    "# Print loaded json object\n",
    "#print(data[0:2])\n",
    "#print(type(data))\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "df_pets = pd.DataFrame.from_dict(json_normalize(data), orient='columns')\n",
    "df_pets.drop(['asin', 'helpful', 'reviewTime', 'reviewerID', 'reviewerName', 'summary', 'unixReviewTime'], axis=1, inplace=True)\n",
    "df_pets.columns = ['Tag', 'Phrase']\n",
    "print(df_pets.shape)\n",
    "print(df_pets.head())\n",
    "print(df_pets.Tag.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# négatif : 1, neutre : 2, 3, 4, positif : 5\n",
    "df_pets_5 = df_pets[df_pets.Tag == 5.0]\n",
    "df_pets_4 = df_pets[df_pets.Tag == 4.0]\n",
    "df_pets_3 = df_pets[df_pets.Tag == 3.0]\n",
    "df_pets_2 = df_pets[df_pets.Tag == 2.0]\n",
    "df_pets_1 = df_pets[df_pets.Tag == 1.0]\n",
    "df_pets_pos_sample = df_pets_5.sample(n=8748, random_state=42)\n",
    "df_pets_neg_sample = df_pets_1.sample(n=8748, random_state=42)\n",
    "df_pets_neu_sample = pd.concat([df_pets_2, df_pets_3, df_pets_4]).sample(n=5000, random_state=42)\n",
    "df_small_pets = pd.concat([df_pets_pos_sample, df_pets_neg_sample, df_pets_neu_sample])\n",
    "df_small_pets['Tag'].replace(1.0, 0, inplace=True)\n",
    "df_small_pets['Tag'].replace(2.0, 0.5, inplace=True)\n",
    "df_small_pets['Tag'].replace(3.0, 0.5, inplace=True)\n",
    "df_small_pets['Tag'].replace(4.0, 0.5, inplace=True)\n",
    "df_small_pets['Tag'].replace(5.0, 1, inplace=True)\n",
    "df_binary_small_pets = pd.concat([df_pets_pos_sample,df_pets_neg_sample])\n",
    "df_binary_small_pets['Tag'].replace(1.0, 0, inplace=True)\n",
    "df_binary_small_pets['Tag'].replace(5.0, 1, inplace=True)\n",
    "df_binary_small_pets.Tag = df_binary_small_pets.Tag.astype(int)\n",
    "#df_small_elec.Tag = df_small_elec.Tag.astype(int)\n",
    "print(df_small_pets.Tag.value_counts())\n",
    "print(df_binary_small_pets.Tag.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_init_pets = df_binary_small_pets.sample(n=15000, random_state=42)\n",
    "test_init_pets = df_binary_small_pets#.loc[~df_binary_small_pets.index.isin(train_init_pets.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_init_pets.shape)\n",
    "print(test_init_pets.Tag.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw_F488eixTV"
   },
   "outputs": [],
   "source": [
    "train = train_init#.sample(25000)\n",
    "test = test_init#_pets#.sample(25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "prRQM8pDi8xI",
    "outputId": "34445cb8-2be0-4379-fdbc-7794091f6049"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tag', 'Phrase'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'Phrase'\n",
    "LABEL_COLUMN = 'Tag'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V399W0rqNJ-Z"
   },
   "source": [
    "#Data Preprocessing\n",
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "- `label` is the label for our example, i.e. True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9gEt5SmM6i6"
   },
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCZWZtKxObjh"
   },
   "source": [
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "\n",
    "\n",
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "Happily, we don't have to worry about most of these details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMWiDtpyQSoU"
   },
   "source": [
    "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IhJSe0QHNG7U",
    "outputId": "20b28cc7-3cb3-4ce6-bfff-a7847ce3bbaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4oFkhpZBDKm"
   },
   "source": [
    "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "dsBo6RCtQmwx",
    "outputId": "9af8c917-90ec-4fe9-897b-79dc89ca88e1"
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OEzfFIt6GIc"
   },
   "source": [
    "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1261
    },
    "colab_type": "code",
    "id": "LL5W8gEGRTAf",
    "outputId": "65001dda-155b-48fc-b5fc-1e4cabc8dfbf"
   },
   "outputs": [],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "#Creating a model\n",
    "\n",
    "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for politeness data.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpE0ZIDOCQzE"
   },
   "source": [
    "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnH-AnOQ9KKW"
   },
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjwJ4bTeWXD8"
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emHf9GhfWBZ_"
   },
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEJldMr3WYZa"
   },
   "outputs": [],
   "source": [
    "# Specify output directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "q_WebpS1X97v",
    "outputId": "1648932a-7391-49d3-8af7-52d514e226e8"
   },
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOO3RfG1DYLo"
   },
   "source": [
    "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Pv2bAlOX_-K"
   },
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6Nukby2EB6-"
   },
   "source": [
    "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "nucD4gluYJmK",
    "outputId": "5d728e72-4631-42bf-c48d-3f51d4b968ce"
   },
   "outputs": [],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmbLTVniARy3"
   },
   "source": [
    "Now let's use our test data to see how well our model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "PPVEXhNjYXC-",
    "outputId": "dd5482cd-c558-465f-c854-ec11a0175316"
   },
   "outputs": [],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueKsULteiz1B"
   },
   "source": [
    "Now let's write code to make predictions on new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsrbTD2EJTVl"
   },
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "    labels = [0, 1]\n",
    "    result = []\n",
    "    t0 = time.time()\n",
    "    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    #print(input_features[0].input_ids)\n",
    "    t1 = time.time()\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    #print(predictions.keys)\n",
    "    t2 = time.time()\n",
    "    #if len(in_sentences)==1:\n",
    "        #for prediction in zip(predictions):\n",
    "     #   prediction = predictions.__next__()\n",
    "      #  return (in_sentences[0], np.exp(prediction['probabilities']), labels[prediction['labels']])\n",
    "    #else:\n",
    "    for sentence, prediction in zip(in_sentences, predictions):\n",
    "        result.append((sentence, np.exp(prediction['probabilities']), labels[prediction['labels']]))\n",
    "    #result.extend((sentence, np.exp(prediction['probabilities']), labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions))\n",
    "    t3 = time.time()\n",
    "    print(\"time total in function: \", t3 - t0)\n",
    "    print(\"time preprocess: \", t1 - t0)\n",
    "    print(\"time preprocess/sentence: \", (t1 - t0)/len(in_sentences))\n",
    "    print(\"time estimator.predict: \", t2-t1)\n",
    "    print(\"time pred: \", t3 - t2)\n",
    "    print(\"time pred/sentence: \", (t3 - t2)/len(in_sentences))\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-thbodgih_VJ"
   },
   "outputs": [],
   "source": [
    "pred_sentences = np.array([\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\",\n",
    "  \"I am happy\",\n",
    "  \"I am happy!!!\"])\n",
    "#sentence = [\"i had few problems with this film and i have heard a lot of criticisms saying it is overlong and overrated .  true    it is over three hours long    but i was amazed that it goes by so quickly .  i don t think it is overrated at all    i think the imdb rating is perfectly decent .  the film looks sumptuous    with gorgeous costumes and excellent effects    and the direction from james cameron rarely slips from focus .  leonardo dicaprio gives one of his best performances as jack    and kate winslet is lovely as rose .  david warner    a great actor    steals every scene he s in .  the story is very rich in detail    and is hot on character development    obvious with the love story which is very moving when it needs to be    though in the first bit of the movie it is a little slow .  the last hour is extremely riveting    and i will confess that i was on the edge of my seat    when the titanic sank .  i will also say that the last five minutes were very moving .  the music score by james horner was lovely    though i never was a huge fan of the song my heart will go on .  the      miniseries was good    but suffered from undeveloped scenarios and some historical inaccuracies .  overall    i give titanic an\"]\n",
    "sentence= np.array([\"The actors were good but the plot was terrible\", \"\"])\n",
    "print(sentence.shape)\n",
    "#sentence.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "QrZmvZySKQTm",
    "outputId": "3891fafb-a460-4eb8-fa6c-335a5bbc10e5"
   },
   "outputs": [],
   "source": [
    "ct = time.time()\n",
    "predictions_result = getPrediction(pred_sentences)\n",
    "print(\"time total: \", time.time()-ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXkRiEBUqN3n"
   },
   "source": [
    "Voila! We have a sentiment classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ERkTE8-7oQLZ",
    "outputId": "26c33224-dc2c-4b3d-f7b4-ac3ef0a58b27"
   },
   "outputs": [],
   "source": [
    "ct = datetime.now()\n",
    "predictions_test = getPrediction(test.Phrase)\n",
    "print(time.time()-ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_test[6787])\n",
    "pred = [predictions_test[i][2] for i in range(len(predictions_test))]\n",
    "print(pred[0:2])\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "print(\"Accuracy: %s\" % accuracy_score(test.Tag, pred))\n",
    "print(\"Precision: %s\" %precision_score(test.Tag, pred))\n",
    "print(\"Recall: %s\" %recall_score(test.Tag, pred))\n",
    "print(\"f1: %s\" %f1_score(test.Tag, pred))\n",
    "print(confusion_matrix(test.Tag, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#errors = pd.DataFrame(index=np.arange(0, len(test.Phrase)), columns=('Phrase', 'Tag', 'Pred'))\n",
    "errors = pd.DataFrame(columns=('Phrase', 'Tag', 'Pred'))\n",
    "c=0\n",
    "for i in range(len(test.Phrase)):\n",
    "    if test.Tag.iloc[i] != predictions_test[i][2]:\n",
    "        errors.loc[len(errors)] = [test.Phrase.iloc[i],  test.Tag.iloc[i], predictions_test[i][2]]\n",
    "        #print(\"diff \", errors.tail())\n",
    "print(errors.shape)\n",
    "print(errors.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.to_csv('errors_bert.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#essai raté\n",
    "feature_spec = {'input_ids': tf.FixedLenFeature([MAX_SEQ_LENGTH],tf.int64), \n",
    "                'input_masks':tf.FixedLenFeature([MAX_SEQ_LENGTH],tf.int64), \n",
    "                'segment_ids':tf.FixedLenFeature([MAX_SEQ_LENGTH],tf.int64),\n",
    "                'label_id': tf.FixedLenFeature([1],tf.int64),\n",
    "                'is_real_example': tf.FixedLenFeature([1],tf.bool)}\n",
    "\n",
    "#def serving_input_receiver_fn():\n",
    " #   serialized_tf_example = tf.placeholder(dtype=tf.string,\n",
    " #                                        shape=[None],\n",
    "  #                                       name='input_tensors')\n",
    "   # receiver_tensors = {'inputs': serialized_tf_example}\n",
    "    \n",
    "    #features = tf.parse_example(serialized_tf_example, feature_spec)\n",
    "    #return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n",
    "\n",
    "def serving_input_receiver_fn():\n",
    "    \"\"\"Serving input_fn that builds features from placeholders\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.estimator.export.ServingInputReceiver\n",
    "    \"\"\"\n",
    "    input_ids = tf.placeholder(dtype=tf.int32, shape=[None,], name='input_ids')\n",
    "    input_mask = tf.placeholder(dtype=tf.int32, shape=[None,], name='input_mask')\n",
    "    segment_ids = tf.placeholder(dtype=tf.int32, shape=[None,], name='segment_ids')\n",
    "    label_ids = tf.placeholder(dtype=tf.int32, shape=[None], name='label_ids')\n",
    "    receiver_tensors = {'input_ids': input_ids, 'input_mask': input_mask, 'segment_ids': segment_ids, 'label_ids': label_ids}\n",
    "    features = {'input_ids': input_ids, 'input_mask': input_mask, 'segment_ids': segment_ids, 'label_ids': label_ids}\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    label_ids = tf.placeholder(tf.int32, [None], name='label_ids')\n",
    "    input_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_ids')\n",
    "    input_mask = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_mask')\n",
    "    segment_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='segment_ids')\n",
    "    input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({\n",
    "        'label_ids': label_ids,\n",
    "        'input_ids': input_ids,\n",
    "        'input_mask': input_mask,\n",
    "        'segment_ids': segment_ids,\n",
    "    })()\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_path = os.path.dirname('.')\n",
    "estimator._export_to_tpu = False\n",
    "estimator.export_savedmodel(dir_path, serving_input_fn, as_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# essais importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_path= os.path.join(dir_path,  \"1551259383\")\n",
    "predictor= tf.contrib.predictor.from_saved_model(exported_path)\n",
    "#tf.contrib not in TF 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in test.Phrase] # here, \"\" is just a dummy label\n",
    "input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "model_input= tf.train.Example(features=tf.train.Features(feature={\n",
    "                'x': tf.train.Feature(float_list=tf.train.FloatList(value=[6.4, 3.2, 4.5, 1.5]))        \n",
    "                })) \n",
    "output_dict= predictor({'input_ids': input_features[0].input_ids, \n",
    "                        'input_mask': input_features[0].input_mask, \n",
    "                        'segment_ids': input_features[0].segment_ids, \n",
    "                        'label_ids': input_features[0].label_id})\n",
    "#output = predictor(input_features[0])\n",
    "print(\" prediction is \" , output_dict['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_features[0].input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "#input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in test.Phrase] # here, \"\" is just a dummy label\n",
    "#input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "# Create the Example\n",
    "def create_example(ex):\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'input_ids': _int64_feature(ex.input_ids),\n",
    "        'input_mask': _int64_feature(ex.input_mask),\n",
    "        'segment_ids': _int64_feature(ex.segment_ids),\n",
    "        'label_ids': tf.train.Feature(\n",
    "            int64_list=tf.train.Int64List(value=[ex.label_ids]))\n",
    "    }))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.python_io.TFRecordWriter('data.tfrecord') as writer:\n",
    "    writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in test.Phrase] # here, \"\" is just a dummy label\n",
    "input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import tag_constants\n",
    "import time\n",
    "def pred(sentences):\n",
    "    export_dir = os.path.join(dir_path,  \"1551259383\")\n",
    "    #predict_file='data.tfrecord'\n",
    "    graph = tf.Graph()\n",
    "    results = []\n",
    "    t0 = time.time()\n",
    "    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in sentences] # here, \"\" is just a dummy label\n",
    "    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    t1 = time.time()\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            tf.saved_model.loader.load(sess, [tag_constants.SERVING], export_dir)\n",
    "        #print(sess.graph.get_operations())\n",
    "        #for op in sess.graph.get_operations():\n",
    "         #   print(op.values())\n",
    "            tensor_input_ids = graph.get_tensor_by_name('input_ids_1:0')\n",
    "            tensor_input_mask = graph.get_tensor_by_name('input_mask_1:0')\n",
    "            tensor_label_ids = graph.get_tensor_by_name('label_ids_1:0')\n",
    "            #print(tensor_label_ids.shape)\n",
    "            tensor_segment_ids = graph.get_tensor_by_name('segment_ids_1:0')\n",
    "            tensor_outputs = graph.get_tensor_by_name('loss/Squeeze:0')\n",
    "        #record_iterator = tf.python_io.tf_record_iterator(path=predict_file)\n",
    "            t2 = time.time()\n",
    "            for i in range(len(sentences)):\n",
    "            #example = tf.train.Example()\n",
    "            #example.ParseFromString(string_record.encode('utf-8'))\n",
    "            #input_ids = example.features.feature['input_ids'].int64_list.value\n",
    "            #input_mask = example.features.feature['input_mask'].int64_list.value\n",
    "            #label_ids = example.features.feature['label_ids'].int64_list.value\n",
    "            #segment_ids = example.features.feature['segment_ids'].int64_list.value\n",
    "                t=time.time()\n",
    "                input_ids = input_features[i].input_ids\n",
    "                input_mask = input_features[i].input_mask\n",
    "                label_ids = input_features[i].label_id\n",
    "                segment_ids = input_features[i].segment_ids\n",
    "                result = sess.run(tensor_outputs, feed_dict={\n",
    "                    tensor_input_ids: np.array(input_ids).reshape(-1, MAX_SEQ_LENGTH),\n",
    "                    tensor_input_mask: np.array(input_mask).reshape(-1, MAX_SEQ_LENGTH),\n",
    "                    tensor_label_ids: np.array(label_ids).reshape(-1, ),\n",
    "                    tensor_segment_ids: np.array(segment_ids).reshape(-1, MAX_SEQ_LENGTH),\n",
    "                })\n",
    "                tbis = time.time()\n",
    "                #print(tbis-t)\n",
    "                results.append(result)\n",
    "        #print((results), sep='\\t')\n",
    "            t3 = time.time()\n",
    "            \n",
    "    print(\"time total in function: \", t3 - t0)\n",
    "    print(\"time preprocess: \", t1 - t0)\n",
    "    print(\"time preprocess/sentence: \", (t1 - t0)/len(sentences))\n",
    "    print(\"time get tensors: \", t2-t1)\n",
    "    print(\"time pred: \", t3 - t2)\n",
    "    print(\"time pred/sentence: \", (t3 - t2)/len(sentences))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = np.array([\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\",\n",
    "  \"it's very funny\",\n",
    "  \"The actors were good but the plot was terrible\"])\n",
    "sentence_long = np.array([\"\", \"i had few problems with this film and i have heard a lot of criticisms saying it is overlong and overrated .  true    it is over three hours long    but i was amazed that it goes by so quickly .  i don t think it is overrated at all    i think the imdb rating is perfectly decent .  the film looks sumptuous    with gorgeous costumes and excellent effects    and the direction from james cameron rarely slips from focus .  leonardo dicaprio gives one of his best performances as jack    and kate winslet is lovely as rose .  david warner    a great actor    steals every scene he s in .  the story is very rich in detail    and is hot on character development    obvious with the love story which is very moving when it needs to be    though in the first bit of the movie it is a little slow .  the last hour is extremely riveting    and i will confess that i was on the edge of my seat    when the titanic sank .  i will also say that the last five minutes were very moving .  the music score by james horner was lovely    though i never was a huge fan of the song my heart will go on .  the      miniseries was good    but suffered from undeveloped scenarios and some historical inaccuracies .  overall    i give titanic an\"])\n",
    "sentence_short = np.array([\"That movie was absolutely awful\"])\n",
    "#sentence= np.array([\"That movie was absolutely awful\"])\n",
    "dir_path = os.path.dirname('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "results = pred(pred_sentences)\n",
    "print(\"total time:\", time.time() - t)\n",
    "print(\"\\n\")\n",
    "t = time.time()\n",
    "results = pred(sentence_short)\n",
    "print(\"total time short:\", time.time() - t)\n",
    "print(results)\n",
    "print(\"\\n\")\n",
    "t = time.time()\n",
    "results = pred(sentence_long)\n",
    "print(\"total time long:\", time.time() - t)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pred(test.Phrase)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "print(\"Accuracy: %s\" % accuracy_score(test.Tag, results))\n",
    "print(\"Precision: %s\" %precision_score(test.Tag, results))\n",
    "print(\"Recall: %s\" %recall_score(test.Tag, results))\n",
    "print(\"f1: %s\" %f1_score(test.Tag, results))\n",
    "print(confusion_matrix(test.Tag, results))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting Movie Reviews with BERT on TF Hub.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
